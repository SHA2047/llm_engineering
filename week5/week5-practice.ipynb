{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Items\n",
    "\n",
    "1. Demonstrate adding context to LLM query and then sending prompt to LLM -- done\n",
    "2. Demonstrate: Loading Documents to langchain document loaders -> Chunking -> Converting to embeddings -> Storing in VectorDB -> Visualizing the embeddings\n",
    "3. Langchain to bring it all together and perform RAG search\n",
    "4. Using FAISS to Show size and Shape of VectorDB and perform similarity Search and again perform RAG search using Langchain\n",
    "5. Proper Investigation i.e. Go Surgical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import glob\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai.api_key:\n",
    "    print(\"API key available\")\n",
    "else:\n",
    "    print(\"API key not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_context = {}\n",
    "\n",
    "directory_structure = glob.glob(\"knowledge-base/**/*.md\", recursive= True)\n",
    "for document in directory_structure:\n",
    "    # print(document.split(\"/\")[-2])\n",
    "    key = document.split(\"/\")[-1].split(\".md\")[0]\n",
    "    print(key)\n",
    "\n",
    "    with open(document, \"r\") as file:\n",
    "        entire_context[key] = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With No Context at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the offerings from Rellm?\"\n",
    "system_message = \"You are an expert in answering accurate questions about Insurellm, the Insurance Tech company. Give brief, accurate answers.\\n\" \\\n",
    "\" If you don't know the answer, say so. Do not make anything up if you haven't been provided with relevant context.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = messages + [{\"role\": \"user\", \"content\": query}]\n",
    "def chat(query, history):\n",
    "    messages = [{\"role\":\"system\", \"content\": system_message}]\n",
    "    messages = messages + [{\"role\": \"user\", \"content\": query}] + history\n",
    "    stream = openai.chat.completions.create(model = MODEL,\n",
    "                                     messages= messages,\n",
    "                                     stream=True)\n",
    "    print(history)\n",
    "    response = ''\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        yield response\n",
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Entire Knowledge Base as Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context(messages, context):\n",
    "    messages = messages + \"\\n\\nThe following context maybe relevant in answering:\\n\\n \" + context\n",
    "    return(messages)\n",
    "\n",
    "def provide_relevant_context(messages, entire_context):\n",
    "    relevant_context = \"\"\n",
    "\n",
    "    for doc, knowledge in entire_context.items():\n",
    "        if doc in messages:\n",
    "            relevant_context += knowledge\n",
    "    \n",
    "    return(relevant_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = messages + [{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "def chat(query, history):\n",
    "    messages = [{\"role\":\"system\", \"content\": system_message}]\n",
    "    query = add_context(query, str(entire_context))\n",
    "    messages = messages + [{\"role\": \"user\", \"content\": query}] + history\n",
    "    stream = openai.chat.completions.create(model = MODEL,\n",
    "                                     messages= messages,\n",
    "                                     stream=True)\n",
    "    print(history)\n",
    "    response = ''\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        yield response\n",
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Relevant Context as Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat(query, history):\n",
    "    messages = [{\"role\":\"system\", \"content\": system_message}]\n",
    "    relevant_context = provide_relevant_context(query, entire_context)\n",
    "    print(f\"Relevant Context: {relevant_context}\")\n",
    "    query = add_context(query, relevant_context)\n",
    "    messages = messages + [{\"role\": \"user\", \"content\": query}] + history\n",
    "    stream = openai.chat.completions.create(model = MODEL,\n",
    "                                     messages= messages,\n",
    "                                     stream=True)\n",
    "    # print(history)\n",
    "    response = ''\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        yield response\n",
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now with langchain and FAISS similarity searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "model.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "loader = DirectoryLoader(\"../\", glob=\"**/*.md\", show_progress=True , silent_errors=True, use_multithreading=True, loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents[50].page_content[:100])\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textsplitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "splitted_documents = textsplitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Check if a Chroma Datastore already exists - if so, delete the collection to start from scratch\n",
    "\n",
    "if os.path.exists(\"vector_db\"):\n",
    "    Chroma(persist_directory=\"vector_db\", embedding_function=embeddings).delete_collection()\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=documents, embedding=OpenAIEmbeddings(), persist_directory=\"vector_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = vector_store._collection\n",
    "sample_embedding = collections.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "len(sample_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hence 66x1536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Vector Embeddings [Skipping for now]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.similarity_search(\"What are the offerings of Rellm?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing it together with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model = MODEL, temperature= 0.9)\n",
    "retriever = vector_store.as_retriever(search_kwargs={'k':10})\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm = model,  retriever = retriever, memory = memory)\n",
    "\n",
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({'question': message})\n",
    "    return(result['answer'])\n",
    "\n",
    "gr.ChatInterface(chat, type = \"messages\").launch(inbrowser = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
